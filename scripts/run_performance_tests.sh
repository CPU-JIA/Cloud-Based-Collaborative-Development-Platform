#!/bin/bash

# Project Service Performance Test Runner
# é¡¹ç›®æœåŠ¡æ€§èƒ½æµ‹è¯•è¿è¡Œè„šæœ¬

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# è„šæœ¬ç›®å½•
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

# é…ç½®æ–‡ä»¶è·¯å¾„
PERFORMANCE_CONFIG="$PROJECT_ROOT/test/performance/performance_config.yaml"
TEST_REPORT_DIR="$PROJECT_ROOT/test-report"

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    cat << EOF
Project Service Performance Test Runner

ç”¨æ³•: $0 [é€‰é¡¹]

é€‰é¡¹:
    -h, --help              æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
    -c, --config FILE       æŒ‡å®šé…ç½®æ–‡ä»¶ (é»˜è®¤: test/performance/performance_config.yaml)
    -o, --output DIR        æŒ‡å®šè¾“å‡ºç›®å½• (é»˜è®¤: test-report)
    -t, --test NAME         è¿è¡Œç‰¹å®šæµ‹è¯• (crud|concurrent|list|mixed|stress|all)
    -u, --users NUM         å¹¶å‘ç”¨æˆ·æ•° (è¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®)
    -d, --duration TIME     æµ‹è¯•æŒç»­æ—¶é—´ (è¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®)
    -v, --verbose           è¯¦ç»†è¾“å‡º
    --cleanup               æµ‹è¯•åæ¸…ç†æ•°æ®
    --no-setup              è·³è¿‡ç¯å¢ƒè®¾ç½®
    --profile               å¯ç”¨æ€§èƒ½åˆ†æ
    --benchmark             è¿è¡ŒåŸºå‡†æµ‹è¯•

ç¤ºä¾‹:
    $0                              # è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•
    $0 -t crud                      # åªè¿è¡ŒCRUDæ€§èƒ½æµ‹è¯•
    $0 -t concurrent -u 100 -d 60s  # è¿è¡Œå¹¶å‘æµ‹è¯•ï¼Œ100ç”¨æˆ·ï¼Œ60ç§’
    $0 --profile --benchmark        # è¿è¡ŒåŸºå‡†æµ‹è¯•å¹¶å¯ç”¨æ€§èƒ½åˆ†æ

EOF
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log_info "æ£€æŸ¥ä¾èµ–..."
    
    # æ£€æŸ¥Go
    if ! command -v go &> /dev/null; then
        log_error "Goæœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­"
        exit 1
    fi
    
    # æ£€æŸ¥PostgreSQLå®¢æˆ·ç«¯
    if ! command -v psql &> /dev/null; then
        log_warning "PostgreSQLå®¢æˆ·ç«¯æœªå®‰è£…ï¼Œæ— æ³•éªŒè¯æ•°æ®åº“è¿æ¥"
    fi
    
    # æ£€æŸ¥å¿…è¦çš„GoåŒ…
    cd "$PROJECT_ROOT"
    if ! go mod verify &> /dev/null; then
        log_error "Goæ¨¡å—éªŒè¯å¤±è´¥"
        exit 1
    fi
    
    log_success "ä¾èµ–æ£€æŸ¥å®Œæˆ"
}

# è®¾ç½®æµ‹è¯•ç¯å¢ƒ
setup_test_environment() {
    log_info "è®¾ç½®æµ‹è¯•ç¯å¢ƒ..."
    
    # åˆ›å»ºæŠ¥å‘Šç›®å½•
    mkdir -p "$TEST_REPORT_DIR"
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    export TEST_DB_HOST="${TEST_DB_HOST:-localhost}"
    export TEST_DB_PORT="${TEST_DB_PORT:-5432}"
    export TEST_DB_USER="${TEST_DB_USER:-test_user}"
    export TEST_DB_PASSWORD="${TEST_DB_PASSWORD:-test_password}"
    export TEST_DB_NAME="${TEST_DB_NAME:-test_db}"
    export GIN_MODE="test"
    export LOG_LEVEL="info"
    
    # éªŒè¯æ•°æ®åº“è¿æ¥
    if command -v psql &> /dev/null; then
        log_info "éªŒè¯æ•°æ®åº“è¿æ¥..."
        if ! PGPASSWORD="$TEST_DB_PASSWORD" psql -h "$TEST_DB_HOST" -p "$TEST_DB_PORT" -U "$TEST_DB_USER" -d "$TEST_DB_NAME" -c "SELECT 1;" &> /dev/null; then
            log_warning "æ— æ³•è¿æ¥åˆ°æµ‹è¯•æ•°æ®åº“ï¼Œè¯·ç¡®ä¿æ•°æ®åº“æ­£åœ¨è¿è¡Œ"
        else
            log_success "æ•°æ®åº“è¿æ¥éªŒè¯æˆåŠŸ"
        fi
    fi
    
    log_success "æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ"
}

# è¿è¡Œç‰¹å®šæ€§èƒ½æµ‹è¯•
run_performance_test() {
    local test_name="$1"
    local additional_args="$2"
    
    log_info "è¿è¡Œæ€§èƒ½æµ‹è¯•: $test_name"
    
    cd "$PROJECT_ROOT"
    
    local test_cmd="go test -v -timeout=30m ./test/performance/"
    
    # æ·»åŠ æµ‹è¯•è¿‡æ»¤å™¨
    case "$test_name" in
        "crud")
            test_cmd+=" -run TestProjectCRUDPerformance"
            ;;
        "concurrent")
            test_cmd+=" -run TestConcurrentProjectCreation"
            ;;
        "list")
            test_cmd+=" -run TestProjectListingPerformance"
            ;;
        "mixed")
            test_cmd+=" -run TestMixedWorkloadPerformance"
            ;;
        "all"|"")
            test_cmd+=" -run TestProjectServicePerformance"
            ;;
        *)
            log_error "æœªçŸ¥çš„æµ‹è¯•ç±»å‹: $test_name"
            return 1
            ;;
    esac
    
    # æ·»åŠ é¢å¤–å‚æ•°
    if [ -n "$additional_args" ]; then
        test_cmd+=" $additional_args"
    fi
    
    # è¿è¡Œæµ‹è¯•
    log_info "æ‰§è¡Œå‘½ä»¤: $test_cmd"
    
    if eval "$test_cmd"; then
        log_success "æ€§èƒ½æµ‹è¯• '$test_name' å®Œæˆ"
        return 0
    else
        log_error "æ€§èƒ½æµ‹è¯• '$test_name' å¤±è´¥"
        return 1
    fi
}

# è¿è¡ŒåŸºå‡†æµ‹è¯•
run_benchmark_test() {
    log_info "è¿è¡ŒåŸºå‡†æµ‹è¯•..."
    
    cd "$PROJECT_ROOT"
    
    local benchmark_cmd="go test -bench=. -benchmem -cpuprofile=cpu.prof -memprofile=mem.prof ./test/performance/"
    
    log_info "æ‰§è¡ŒåŸºå‡†æµ‹è¯•å‘½ä»¤: $benchmark_cmd"
    
    if eval "$benchmark_cmd"; then
        log_success "åŸºå‡†æµ‹è¯•å®Œæˆ"
        
        # ç§»åŠ¨æ€§èƒ½åˆ†ææ–‡ä»¶åˆ°æŠ¥å‘Šç›®å½•
        if [ -f "cpu.prof" ]; then
            mv cpu.prof "$TEST_REPORT_DIR/"
            log_info "CPUæ€§èƒ½åˆ†ææ–‡ä»¶å·²ä¿å­˜åˆ° $TEST_REPORT_DIR/cpu.prof"
        fi
        
        if [ -f "mem.prof" ]; then
            mv mem.prof "$TEST_REPORT_DIR/"
            log_info "å†…å­˜æ€§èƒ½åˆ†ææ–‡ä»¶å·²ä¿å­˜åˆ° $TEST_REPORT_DIR/mem.prof"
        fi
        
        return 0
    else
        log_error "åŸºå‡†æµ‹è¯•å¤±è´¥"
        return 1
    fi
}

# ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
generate_performance_html_report() {
    log_info "ç”ŸæˆHTMLæ€§èƒ½æŠ¥å‘Š..."
    
    local json_report="$TEST_REPORT_DIR/project_service_performance_report.json"
    local html_report="$TEST_REPORT_DIR/project_service_performance_report.html"
    
    if [ ! -f "$json_report" ]; then
        log_warning "JSONæŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡HTMLæŠ¥å‘Šç”Ÿæˆ"
        return 0
    fi
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    cat > "$html_report" << 'EOF'
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Service Performance Test Report</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1, h2, h3 {
            color: #333;
            border-bottom: 2px solid #007cba;
            padding-bottom: 10px;
        }
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        .metric-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            border-left: 4px solid #007cba;
        }
        .metric-value {
            font-size: 2em;
            font-weight: bold;
            color: #007cba;
        }
        .metric-label {
            color: #666;
            text-transform: uppercase;
            font-size: 0.9em;
        }
        .chart-container {
            width: 100%;
            height: 400px;
            margin: 20px 0;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #007cba;
            color: white;
        }
        tr:hover {
            background-color: #f5f5f5;
        }
        .success {
            color: #28a745;
        }
        .warning {
            color: #ffc107;
        }
        .error {
            color: #dc3545;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š Project Service Performance Test Report</h1>
        <p>Generated at: <span id="timestamp"></span></p>
        
        <h2>ğŸ¯ Overall Performance Metrics</h2>
        <div class="metrics-grid" id="overall-metrics">
            <!-- åŠ¨æ€ç”Ÿæˆ -->
        </div>
        
        <h2>ğŸ“ˆ Response Time Distribution</h2>
        <div class="chart-container">
            <canvas id="responseTimeChart"></canvas>
        </div>
        
        <h2>ğŸ”— Endpoint Performance</h2>
        <table id="endpoint-table">
            <thead>
                <tr>
                    <th>Endpoint</th>
                    <th>Requests</th>
                    <th>Avg Time</th>
                    <th>Min Time</th>
                    <th>Max Time</th>
                    <th>Success Rate</th>
                    <th>Status</th>
                </tr>
            </thead>
            <tbody id="endpoint-tbody">
                <!-- åŠ¨æ€ç”Ÿæˆ -->
            </tbody>
        </table>
        
        <h2>âš ï¸ Error Distribution</h2>
        <table id="error-table">
            <thead>
                <tr>
                    <th>Status Code</th>
                    <th>Count</th>
                    <th>Percentage</th>
                </tr>
            </thead>
            <tbody id="error-tbody">
                <!-- åŠ¨æ€ç”Ÿæˆ -->
            </tbody>
        </table>
    </div>

    <script>
        // è®¾ç½®æ—¶é—´æˆ³
        document.getElementById('timestamp').textContent = new Date().toLocaleString();
        
        // åŠ è½½æ€§èƒ½æ•°æ®
        fetch('./project_service_performance_report.json')
            .then(response => response.json())
            .then(data => {
                renderOverallMetrics(data);
                renderEndpointTable(data);
                renderErrorTable(data);
                renderResponseTimeChart(data);
            })
            .catch(error => {
                console.error('Error loading performance data:', error);
                document.querySelector('.container').innerHTML += 
                    '<div class="error">Error loading performance data. Please ensure the JSON report exists.</div>';
            });
        
        function renderOverallMetrics(data) {
            const container = document.getElementById('overall-metrics');
            const metrics = [
                { label: 'Total Requests', value: data.request_count, suffix: '' },
                { label: 'Success Rate', value: data.success_rate?.toFixed(1) || '0.0', suffix: '%' },
                { label: 'Avg Response Time', value: formatDuration(data.average_response_time), suffix: '' },
                { label: 'P95 Response Time', value: formatDuration(data.p95_response_time), suffix: '' },
                { label: 'P99 Response Time', value: formatDuration(data.p99_response_time), suffix: '' },
                { label: 'Throughput', value: data.throughput_rps?.toFixed(1) || '0.0', suffix: ' RPS' }
            ];
            
            container.innerHTML = metrics.map(metric => `
                <div class="metric-card">
                    <div class="metric-value">${metric.value}${metric.suffix}</div>
                    <div class="metric-label">${metric.label}</div>
                </div>
            `).join('');
        }
        
        function renderEndpointTable(data) {
            const tbody = document.getElementById('endpoint-tbody');
            if (!data.endpoint_metrics) return;
            
            const rows = Object.entries(data.endpoint_metrics).map(([endpoint, metrics]) => {
                const statusClass = metrics.success_rate >= 95 ? 'success' : 
                                  metrics.success_rate >= 90 ? 'warning' : 'error';
                const status = metrics.success_rate >= 95 ? 'âœ…' : 
                              metrics.success_rate >= 90 ? 'âš ï¸' : 'âŒ';
                
                return `
                    <tr>
                        <td>${endpoint}</td>
                        <td>${metrics.request_count}</td>
                        <td>${formatDuration(metrics.average_time)}</td>
                        <td>${formatDuration(metrics.min_time)}</td>
                        <td>${formatDuration(metrics.max_time)}</td>
                        <td class="${statusClass}">${metrics.success_rate?.toFixed(1) || '0.0'}%</td>
                        <td>${status}</td>
                    </tr>
                `;
            }).join('');
            
            tbody.innerHTML = rows;
        }
        
        function renderErrorTable(data) {
            const tbody = document.getElementById('error-tbody');
            if (!data.error_distribution) return;
            
            const totalErrors = Object.values(data.error_distribution).reduce((sum, count) => sum + count, 0);
            
            const rows = Object.entries(data.error_distribution).map(([statusCode, count]) => {
                const percentage = ((count / totalErrors) * 100).toFixed(1);
                return `
                    <tr>
                        <td>${statusCode}</td>
                        <td>${count}</td>
                        <td>${percentage}%</td>
                    </tr>
                `;
            }).join('');
            
            tbody.innerHTML = rows || '<tr><td colspan="3">No errors detected</td></tr>';
        }
        
        function renderResponseTimeChart(data) {
            // è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„å›¾è¡¨é€»è¾‘
            // ç”±äºç®€åŒ–ï¼Œæš‚æ—¶æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            const ctx = document.getElementById('responseTimeChart').getContext('2d');
            
            // æ¨¡æ‹Ÿå“åº”æ—¶é—´åˆ†å¸ƒæ•°æ®
            new Chart(ctx, {
                type: 'line',
                data: {
                    labels: ['0-100ms', '100-200ms', '200-500ms', '500ms-1s', '1s+'],
                    datasets: [{
                        label: 'Response Time Distribution',
                        data: [40, 30, 20, 8, 2], // ç¤ºä¾‹æ•°æ®
                        borderColor: '#007cba',
                        backgroundColor: 'rgba(0, 124, 186, 0.1)',
                        tension: 0.4
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: 'Request Count'
                            }
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'Response Time Range'
                            }
                        }
                    }
                }
            });
        }
        
        function formatDuration(nanoseconds) {
            if (!nanoseconds) return '0ms';
            const ms = nanoseconds / 1000000;
            if (ms < 1000) {
                return `${ms.toFixed(1)}ms`;
            } else {
                return `${(ms / 1000).toFixed(2)}s`;
            }
        }
    </script>
</body>
</html>
EOF

    log_success "HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: $html_report"
}

# æ¸…ç†æµ‹è¯•æ•°æ®
cleanup_test_data() {
    log_info "æ¸…ç†æµ‹è¯•æ•°æ®..."
    
    if command -v psql &> /dev/null; then
        # æ¸…ç†æ•°æ®åº“ä¸­çš„æµ‹è¯•æ•°æ®
        PGPASSWORD="$TEST_DB_PASSWORD" psql -h "$TEST_DB_HOST" -p "$TEST_DB_PORT" -U "$TEST_DB_USER" -d "$TEST_DB_NAME" << EOF
DELETE FROM projects WHERE key LIKE 'perf-test-%' OR key LIKE 'load-test-%' OR key LIKE 'list-perf-%' OR key LIKE 'mixed-%';
DELETE FROM project_members WHERE project_id IN (SELECT id FROM projects WHERE key LIKE '%test%');
DELETE FROM repositories WHERE project_id IN (SELECT id FROM projects WHERE key LIKE '%test%');
EOF
    fi
    
    log_success "æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ"
}

# ä¸»å‡½æ•°
main() {
    local config_file="$PERFORMANCE_CONFIG"
    local output_dir="$TEST_REPORT_DIR"
    local test_name="all"
    local concurrent_users=""
    local duration=""
    local verbose=false
    local cleanup=false
    local no_setup=false
    local profile=false
    local benchmark=false
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                show_help
                exit 0
                ;;
            -c|--config)
                config_file="$2"
                shift 2
                ;;
            -o|--output)
                output_dir="$2"
                shift 2
                ;;
            -t|--test)
                test_name="$2"
                shift 2
                ;;
            -u|--users)
                concurrent_users="$2"
                shift 2
                ;;
            -d|--duration)
                duration="$2"
                shift 2
                ;;
            -v|--verbose)
                verbose=true
                shift
                ;;
            --cleanup)
                cleanup=true
                shift
                ;;
            --no-setup)
                no_setup=true
                shift
                ;;
            --profile)
                profile=true
                shift
                ;;
            --benchmark)
                benchmark=true
                shift
                ;;
            *)
                log_error "æœªçŸ¥é€‰é¡¹: $1"
                show_help
                exit 1
                ;;
        esac
    done
    
    # è®¾ç½®è¯¦ç»†è¾“å‡º
    if [ "$verbose" = true ]; then
        set -x
    fi
    
    log_info "é¡¹ç›®æœåŠ¡æ€§èƒ½æµ‹è¯•å¼€å§‹"
    log_info "é…ç½®æ–‡ä»¶: $config_file"
    log_info "è¾“å‡ºç›®å½•: $output_dir"
    log_info "æµ‹è¯•ç±»å‹: $test_name"
    
    # æ£€æŸ¥ä¾èµ–
    check_dependencies
    
    # è®¾ç½®ç¯å¢ƒ
    if [ "$no_setup" = false ]; then
        setup_test_environment
    fi
    
    # æ„å»ºé¢å¤–å‚æ•°
    local additional_args=""
    if [ -n "$concurrent_users" ]; then
        additional_args+=" -args -users=$concurrent_users"
    fi
    if [ -n "$duration" ]; then
        additional_args+=" -duration=$duration"
    fi
    
    # è¿è¡Œæµ‹è¯•
    local test_failed=false
    
    if [ "$benchmark" = true ]; then
        if ! run_benchmark_test; then
            test_failed=true
        fi
    else
        if ! run_performance_test "$test_name" "$additional_args"; then
            test_failed=true
        fi
    fi
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_performance_html_report
    
    # æ¸…ç†
    if [ "$cleanup" = true ]; then
        cleanup_test_data
    fi
    
    # è¾“å‡ºç»“æœ
    if [ "$test_failed" = true ]; then
        log_error "æ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥"
        exit 1
    else
        log_success "æ€§èƒ½æµ‹è¯•æ‰§è¡Œå®Œæˆ"
        log_info "æŸ¥çœ‹æŠ¥å‘Š: $output_dir/project_service_performance_report.html"
        exit 0
    fi
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"