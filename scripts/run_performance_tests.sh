#!/bin/bash

# Project Service Performance Test Runner
# È°πÁõÆÊúçÂä°ÊÄßËÉΩÊµãËØïËøêË°åËÑöÊú¨

set -e

# È¢úËâ≤ÂÆö‰πâ
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# ËÑöÊú¨ÁõÆÂΩï
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

# ÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ
PERFORMANCE_CONFIG="$PROJECT_ROOT/test/performance/performance_config.yaml"
TEST_REPORT_DIR="$PROJECT_ROOT/test-report"

# Êó•ÂøóÂáΩÊï∞
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# ÊòæÁ§∫Â∏ÆÂä©‰ø°ÊÅØ
show_help() {
    cat << EOF
Project Service Performance Test Runner

Áî®Ê≥ï: $0 [ÈÄâÈ°π]

ÈÄâÈ°π:
    -h, --help              ÊòæÁ§∫Ê≠§Â∏ÆÂä©‰ø°ÊÅØ
    -c, --config FILE       ÊåáÂÆöÈÖçÁΩÆÊñá‰ª∂ (ÈªòËÆ§: test/performance/performance_config.yaml)
    -o, --output DIR        ÊåáÂÆöËæìÂá∫ÁõÆÂΩï (ÈªòËÆ§: test-report)
    -t, --test NAME         ËøêË°åÁâπÂÆöÊµãËØï (crud|concurrent|list|mixed|stress|all)
    -u, --users NUM         Âπ∂ÂèëÁî®Êà∑Êï∞ (Ë¶ÜÁõñÈÖçÁΩÆÊñá‰ª∂ËÆæÁΩÆ)
    -d, --duration TIME     ÊµãËØïÊåÅÁª≠Êó∂Èó¥ (Ë¶ÜÁõñÈÖçÁΩÆÊñá‰ª∂ËÆæÁΩÆ)
    -v, --verbose           ËØ¶ÁªÜËæìÂá∫
    --cleanup               ÊµãËØïÂêéÊ∏ÖÁêÜÊï∞ÊçÆ
    --no-setup              Ë∑≥ËøáÁéØÂ¢ÉËÆæÁΩÆ
    --profile               ÂêØÁî®ÊÄßËÉΩÂàÜÊûê
    --benchmark             ËøêË°åÂü∫ÂáÜÊµãËØï

Á§∫‰æã:
    $0                              # ËøêË°åÊâÄÊúâÊÄßËÉΩÊµãËØï
    $0 -t crud                      # Âè™ËøêË°åCRUDÊÄßËÉΩÊµãËØï
    $0 -t concurrent -u 100 -d 60s  # ËøêË°åÂπ∂ÂèëÊµãËØïÔºå100Áî®Êà∑Ôºå60Áßí
    $0 --profile --benchmark        # ËøêË°åÂü∫ÂáÜÊµãËØïÂπ∂ÂêØÁî®ÊÄßËÉΩÂàÜÊûê

EOF
}

# Ê£ÄÊü•‰æùËµñ
check_dependencies() {
    log_info "Ê£ÄÊü•‰æùËµñ..."
    
    # Ê£ÄÊü•Go
    if ! command -v go &> /dev/null; then
        log_error "GoÊú™ÂÆâË£ÖÊàñ‰∏çÂú®PATH‰∏≠"
        exit 1
    fi
    
    # Ê£ÄÊü•PostgreSQLÂÆ¢Êà∑Á´Ø
    if ! command -v psql &> /dev/null; then
        log_warning "PostgreSQLÂÆ¢Êà∑Á´ØÊú™ÂÆâË£ÖÔºåÊó†Ê≥ïÈ™åËØÅÊï∞ÊçÆÂ∫ìËøûÊé•"
    fi
    
    # Ê£ÄÊü•ÂøÖË¶ÅÁöÑGoÂåÖ
    cd "$PROJECT_ROOT"
    if ! go mod verify &> /dev/null; then
        log_error "GoÊ®°ÂùóÈ™åËØÅÂ§±Ë¥•"
        exit 1
    fi
    
    log_success "‰æùËµñÊ£ÄÊü•ÂÆåÊàê"
}

# ËÆæÁΩÆÊµãËØïÁéØÂ¢É
setup_test_environment() {
    log_info "ËÆæÁΩÆÊµãËØïÁéØÂ¢É..."
    
    # ÂàõÂª∫Êä•ÂëäÁõÆÂΩï
    mkdir -p "$TEST_REPORT_DIR"
    
    # ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè
    export TEST_DB_HOST="${TEST_DB_HOST:-localhost}"
    export TEST_DB_PORT="${TEST_DB_PORT:-5432}"
    export TEST_DB_USER="${TEST_DB_USER:-test_user}"
    export TEST_DB_PASSWORD="${TEST_DB_PASSWORD:-test_password}"
    export TEST_DB_NAME="${TEST_DB_NAME:-test_db}"
    export GIN_MODE="test"
    export LOG_LEVEL="info"
    
    # È™åËØÅÊï∞ÊçÆÂ∫ìËøûÊé•
    if command -v psql &> /dev/null; then
        log_info "È™åËØÅÊï∞ÊçÆÂ∫ìËøûÊé•..."
        if ! PGPASSWORD="$TEST_DB_PASSWORD" psql -h "$TEST_DB_HOST" -p "$TEST_DB_PORT" -U "$TEST_DB_USER" -d "$TEST_DB_NAME" -c "SELECT 1;" &> /dev/null; then
            log_warning "Êó†Ê≥ïËøûÊé•Âà∞ÊµãËØïÊï∞ÊçÆÂ∫ìÔºåËØ∑Á°Æ‰øùÊï∞ÊçÆÂ∫ìÊ≠£Âú®ËøêË°å"
        else
            log_success "Êï∞ÊçÆÂ∫ìËøûÊé•È™åËØÅÊàêÂäü"
        fi
    fi
    
    log_success "ÊµãËØïÁéØÂ¢ÉËÆæÁΩÆÂÆåÊàê"
}

# ËøêË°åÁâπÂÆöÊÄßËÉΩÊµãËØï
run_performance_test() {
    local test_name="$1"
    local additional_args="$2"
    
    log_info "ËøêË°åÊÄßËÉΩÊµãËØï: $test_name"
    
    cd "$PROJECT_ROOT"
    
    local test_cmd="go test -v -timeout=30m ./test/performance/"
    
    # Ê∑ªÂä†ÊµãËØïËøáÊª§Âô®
    case "$test_name" in
        "crud")
            test_cmd+=" -run TestProjectCRUDPerformance"
            ;;
        "concurrent")
            test_cmd+=" -run TestConcurrentProjectCreation"
            ;;
        "list")
            test_cmd+=" -run TestProjectListingPerformance"
            ;;
        "mixed")
            test_cmd+=" -run TestMixedWorkloadPerformance"
            ;;
        "all"|"")
            test_cmd+=" -run TestProjectServicePerformance"
            ;;
        *)
            log_error "Êú™Áü•ÁöÑÊµãËØïÁ±ªÂûã: $test_name"
            return 1
            ;;
    esac
    
    # Ê∑ªÂä†È¢ùÂ§ñÂèÇÊï∞
    if [ -n "$additional_args" ]; then
        test_cmd+=" $additional_args"
    fi
    
    # ËøêË°åÊµãËØï
    log_info "ÊâßË°åÂëΩ‰ª§: $test_cmd"
    
    if eval "$test_cmd"; then
        log_success "ÊÄßËÉΩÊµãËØï '$test_name' ÂÆåÊàê"
        return 0
    else
        log_error "ÊÄßËÉΩÊµãËØï '$test_name' Â§±Ë¥•"
        return 1
    fi
}

# ËøêË°åÂü∫ÂáÜÊµãËØï
run_benchmark_test() {
    log_info "ËøêË°åÂü∫ÂáÜÊµãËØï..."
    
    cd "$PROJECT_ROOT"
    
    local benchmark_cmd="go test -bench=. -benchmem -cpuprofile=cpu.prof -memprofile=mem.prof ./test/performance/"
    
    log_info "ÊâßË°åÂü∫ÂáÜÊµãËØïÂëΩ‰ª§: $benchmark_cmd"
    
    if eval "$benchmark_cmd"; then
        log_success "Âü∫ÂáÜÊµãËØïÂÆåÊàê"
        
        # ÁßªÂä®ÊÄßËÉΩÂàÜÊûêÊñá‰ª∂Âà∞Êä•ÂëäÁõÆÂΩï
        if [ -f "cpu.prof" ]; then
            mv cpu.prof "$TEST_REPORT_DIR/"
            log_info "CPUÊÄßËÉΩÂàÜÊûêÊñá‰ª∂Â∑≤‰øùÂ≠òÂà∞ $TEST_REPORT_DIR/cpu.prof"
        fi
        
        if [ -f "mem.prof" ]; then
            mv mem.prof "$TEST_REPORT_DIR/"
            log_info "ÂÜÖÂ≠òÊÄßËÉΩÂàÜÊûêÊñá‰ª∂Â∑≤‰øùÂ≠òÂà∞ $TEST_REPORT_DIR/mem.prof"
        fi
        
        return 0
    else
        log_error "Âü∫ÂáÜÊµãËØïÂ§±Ë¥•"
        return 1
    fi
}

# ÁîüÊàêÊÄßËÉΩÊä•Âëä
generate_performance_html_report() {
    log_info "ÁîüÊàêHTMLÊÄßËÉΩÊä•Âëä..."
    
    local json_report="$TEST_REPORT_DIR/project_service_performance_report.json"
    local html_report="$TEST_REPORT_DIR/project_service_performance_report.html"
    
    if [ ! -f "$json_report" ]; then
        log_warning "JSONÊä•ÂëäÊñá‰ª∂‰∏çÂ≠òÂú®ÔºåË∑≥ËøáHTMLÊä•ÂëäÁîüÊàê"
        return 0
    fi
    
    # ÁîüÊàêHTMLÊä•Âëä
    cat > "$html_report" << 'EOF'
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Service Performance Test Report</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1, h2, h3 {
            color: #333;
            border-bottom: 2px solid #007cba;
            padding-bottom: 10px;
        }
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        .metric-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            border-left: 4px solid #007cba;
        }
        .metric-value {
            font-size: 2em;
            font-weight: bold;
            color: #007cba;
        }
        .metric-label {
            color: #666;
            text-transform: uppercase;
            font-size: 0.9em;
        }
        .chart-container {
            width: 100%;
            height: 400px;
            margin: 20px 0;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #007cba;
            color: white;
        }
        tr:hover {
            background-color: #f5f5f5;
        }
        .success {
            color: #28a745;
        }
        .warning {
            color: #ffc107;
        }
        .error {
            color: #dc3545;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üìä Project Service Performance Test Report</h1>
        <p>Generated at: <span id="timestamp"></span></p>
        
        <h2>üéØ Overall Performance Metrics</h2>
        <div class="metrics-grid" id="overall-metrics">
            <!-- Âä®ÊÄÅÁîüÊàê -->
        </div>
        
        <h2>üìà Response Time Distribution</h2>
        <div class="chart-container">
            <canvas id="responseTimeChart"></canvas>
        </div>
        
        <h2>üîó Endpoint Performance</h2>
        <table id="endpoint-table">
            <thead>
                <tr>
                    <th>Endpoint</th>
                    <th>Requests</th>
                    <th>Avg Time</th>
                    <th>Min Time</th>
                    <th>Max Time</th>
                    <th>Success Rate</th>
                    <th>Status</th>
                </tr>
            </thead>
            <tbody id="endpoint-tbody">
                <!-- Âä®ÊÄÅÁîüÊàê -->
            </tbody>
        </table>
        
        <h2>‚ö†Ô∏è Error Distribution</h2>
        <table id="error-table">
            <thead>
                <tr>
                    <th>Status Code</th>
                    <th>Count</th>
                    <th>Percentage</th>
                </tr>
            </thead>
            <tbody id="error-tbody">
                <!-- Âä®ÊÄÅÁîüÊàê -->
            </tbody>
        </table>
    </div>

    <script>
        // ËÆæÁΩÆÊó∂Èó¥Êà≥
        document.getElementById('timestamp').textContent = new Date().toLocaleString();
        
        // Âä†ËΩΩÊÄßËÉΩÊï∞ÊçÆ
        fetch('./project_service_performance_report.json')
            .then(response => response.json())
            .then(data => {
                renderOverallMetrics(data);
                renderEndpointTable(data);
                renderErrorTable(data);
                renderResponseTimeChart(data);
            })
            .catch(error => {
                console.error('Error loading performance data:', error);
                document.querySelector('.container').innerHTML += 
                    '<div class="error">Error loading performance data. Please ensure the JSON report exists.</div>';
            });
        
        function renderOverallMetrics(data) {
            const container = document.getElementById('overall-metrics');
            const metrics = [
                { label: 'Total Requests', value: data.request_count, suffix: '' },
                { label: 'Success Rate', value: data.success_rate?.toFixed(1) || '0.0', suffix: '%' },
                { label: 'Avg Response Time', value: formatDuration(data.average_response_time), suffix: '' },
                { label: 'P95 Response Time', value: formatDuration(data.p95_response_time), suffix: '' },
                { label: 'P99 Response Time', value: formatDuration(data.p99_response_time), suffix: '' },
                { label: 'Throughput', value: data.throughput_rps?.toFixed(1) || '0.0', suffix: ' RPS' }
            ];
            
            container.innerHTML = metrics.map(metric => `
                <div class="metric-card">
                    <div class="metric-value">${metric.value}${metric.suffix}</div>
                    <div class="metric-label">${metric.label}</div>
                </div>
            `).join('');
        }
        
        function renderEndpointTable(data) {
            const tbody = document.getElementById('endpoint-tbody');
            if (!data.endpoint_metrics) return;
            
            const rows = Object.entries(data.endpoint_metrics).map(([endpoint, metrics]) => {
                const statusClass = metrics.success_rate >= 95 ? 'success' : 
                                  metrics.success_rate >= 90 ? 'warning' : 'error';
                const status = metrics.success_rate >= 95 ? '‚úÖ' : 
                              metrics.success_rate >= 90 ? '‚ö†Ô∏è' : '‚ùå';
                
                return `
                    <tr>
                        <td>${endpoint}</td>
                        <td>${metrics.request_count}</td>
                        <td>${formatDuration(metrics.average_time)}</td>
                        <td>${formatDuration(metrics.min_time)}</td>
                        <td>${formatDuration(metrics.max_time)}</td>
                        <td class="${statusClass}">${metrics.success_rate?.toFixed(1) || '0.0'}%</td>
                        <td>${status}</td>
                    </tr>
                `;
            }).join('');
            
            tbody.innerHTML = rows;
        }
        
        function renderErrorTable(data) {
            const tbody = document.getElementById('error-tbody');
            if (!data.error_distribution) return;
            
            const totalErrors = Object.values(data.error_distribution).reduce((sum, count) => sum + count, 0);
            
            const rows = Object.entries(data.error_distribution).map(([statusCode, count]) => {
                const percentage = ((count / totalErrors) * 100).toFixed(1);
                return `
                    <tr>
                        <td>${statusCode}</td>
                        <td>${count}</td>
                        <td>${percentage}%</td>
                    </tr>
                `;
            }).join('');
            
            tbody.innerHTML = rows || '<tr><td colspan="3">No errors detected</td></tr>';
        }
        
        function renderResponseTimeChart(data) {
            // ËøôÈáåÂèØ‰ª•Ê∑ªÂä†Êõ¥Â§çÊùÇÁöÑÂõæË°®ÈÄªËæë
            // Áî±‰∫éÁÆÄÂåñÔºåÊöÇÊó∂ÊòæÁ§∫Âü∫Êú¨‰ø°ÊÅØ
            const ctx = document.getElementById('responseTimeChart').getContext('2d');
            
            // Ê®°ÊãüÂìçÂ∫îÊó∂Èó¥ÂàÜÂ∏ÉÊï∞ÊçÆ
            new Chart(ctx, {
                type: 'line',
                data: {
                    labels: ['0-100ms', '100-200ms', '200-500ms', '500ms-1s', '1s+'],
                    datasets: [{
                        label: 'Response Time Distribution',
                        data: [40, 30, 20, 8, 2], // Á§∫‰æãÊï∞ÊçÆ
                        borderColor: '#007cba',
                        backgroundColor: 'rgba(0, 124, 186, 0.1)',
                        tension: 0.4
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: 'Request Count'
                            }
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'Response Time Range'
                            }
                        }
                    }
                }
            });
        }
        
        function formatDuration(nanoseconds) {
            if (!nanoseconds) return '0ms';
            const ms = nanoseconds / 1000000;
            if (ms < 1000) {
                return `${ms.toFixed(1)}ms`;
            } else {
                return `${(ms / 1000).toFixed(2)}s`;
            }
        }
    </script>
</body>
</html>
EOF

    log_success "HTMLÊä•ÂëäÂ∑≤ÁîüÊàê: $html_report"
}

# Ê∏ÖÁêÜÊµãËØïÊï∞ÊçÆ
cleanup_test_data() {
    log_info "Ê∏ÖÁêÜÊµãËØïÊï∞ÊçÆ..."
    
    if command -v psql &> /dev/null; then
        # Ê∏ÖÁêÜÊï∞ÊçÆÂ∫ì‰∏≠ÁöÑÊµãËØïÊï∞ÊçÆ
        PGPASSWORD="$TEST_DB_PASSWORD" psql -h "$TEST_DB_HOST" -p "$TEST_DB_PORT" -U "$TEST_DB_USER" -d "$TEST_DB_NAME" << EOF
DELETE FROM projects WHERE key LIKE 'perf-test-%' OR key LIKE 'load-test-%' OR key LIKE 'list-perf-%' OR key LIKE 'mixed-%';
DELETE FROM project_members WHERE project_id IN (SELECT id FROM projects WHERE key LIKE '%test%');
DELETE FROM repositories WHERE project_id IN (SELECT id FROM projects WHERE key LIKE '%test%');
EOF
    fi
    
    log_success "ÊµãËØïÊï∞ÊçÆÊ∏ÖÁêÜÂÆåÊàê"
}

# ‰∏ªÂáΩÊï∞
main() {
    local config_file="$PERFORMANCE_CONFIG"
    local output_dir="$TEST_REPORT_DIR"
    local test_name="all"
    local concurrent_users=""
    local duration=""
    local verbose=false
    local cleanup=false
    local no_setup=false
    local profile=false
    local benchmark=false
    
    # Ëß£ÊûêÂëΩ‰ª§Ë°åÂèÇÊï∞
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                show_help
                exit 0
                ;;
            -c|--config)
                config_file="$2"
                shift 2
                ;;
            -o|--output)
                output_dir="$2"
                shift 2
                ;;
            -t|--test)
                test_name="$2"
                shift 2
                ;;
            -u|--users)
                concurrent_users="$2"
                shift 2
                ;;
            -d|--duration)
                duration="$2"
                shift 2
                ;;
            -v|--verbose)
                verbose=true
                shift
                ;;
            --cleanup)
                cleanup=true
                shift
                ;;
            --no-setup)
                no_setup=true
                shift
                ;;
            --profile)
                profile=true
                shift
                ;;
            --benchmark)
                benchmark=true
                shift
                ;;
            *)
                log_error "Êú™Áü•ÈÄâÈ°π: $1"
                show_help
                exit 1
                ;;
        esac
    done
    
    # ËÆæÁΩÆËØ¶ÁªÜËæìÂá∫
    if [ "$verbose" = true ]; then
        set -x
    fi
    
    log_info "È°πÁõÆÊúçÂä°ÊÄßËÉΩÊµãËØïÂºÄÂßã"
    log_info "ÈÖçÁΩÆÊñá‰ª∂: $config_file"
    log_info "ËæìÂá∫ÁõÆÂΩï: $output_dir"
    log_info "ÊµãËØïÁ±ªÂûã: $test_name"
    
    # Ê£ÄÊü•‰æùËµñ
    check_dependencies
    
    # ËÆæÁΩÆÁéØÂ¢É
    if [ "$no_setup" = false ]; then
        setup_test_environment
    fi
    
    # ÊûÑÂª∫È¢ùÂ§ñÂèÇÊï∞
    local additional_args=""
    if [ -n "$concurrent_users" ]; then
        additional_args+=" -args -users=$concurrent_users"
    fi
    if [ -n "$duration" ]; then
        additional_args+=" -duration=$duration"
    fi
    
    # ËøêË°åÊµãËØï
    local test_failed=false
    
    if [ "$benchmark" = true ]; then
        if ! run_benchmark_test; then
            test_failed=true
        fi
    else
        if ! run_performance_test "$test_name" "$additional_args"; then
            test_failed=true
        fi
    fi
    
    # ÁîüÊàêÊä•Âëä
    generate_performance_html_report
    
    # Ê∏ÖÁêÜ
    if [ "$cleanup" = true ]; then
        cleanup_test_data
    fi
    
    # ËæìÂá∫ÁªìÊûú
    if [ "$test_failed" = true ]; then
        log_error "ÊÄßËÉΩÊµãËØïÊâßË°åÂ§±Ë¥•"
        exit 1
    else
        log_success "ÊÄßËÉΩÊµãËØïÊâßË°åÂÆåÊàê"
        log_info "Êü•ÁúãÊä•Âëä: $output_dir/project_service_performance_report.html"
        exit 0
    fi
}

# ÊâßË°å‰∏ªÂáΩÊï∞
main "$@"